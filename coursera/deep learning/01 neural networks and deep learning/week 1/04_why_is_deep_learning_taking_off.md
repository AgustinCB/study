# Why is deep learning taking off?

If the basic technical ideas behind deep learning, why is taking off now? We will cover the reason this lesson.

If we plot the performance of a traditional learning algorithm with respect to the amount of data, you will see that the performance increases at the beginning with more data, but after a while it really stops making a difference.

In our society, we passed from having very small amounts of data to have huge amounts of it. Over the last twenty years we accumulated much more data than our traditional algorithm can use.

If you plot neural networks' performance with respect to the amount of (labeled) data you will notice that the larger the neural network is the most performance it can get from more data. With this, we know that we can scale a machine learning problem by either increasing the number of nodes in a neural network or increasing the amount of labeled data.

Also, on small training sets other algorithms may perform better. Is just when you have really huge amounts of data that neural networks make a difference.

We can say then that scale drives deep learning progress. In the last years we have collected more data, improved our computation techniques and the algorithms that we use (for example, the switch from sigmoid functions to relu functions) and that improved the performance of neural networks.

The process of training a neural network is iterative, in the sense that you run a test, experiment, refine your dea and write code again. Because of this, there's a huge improvement in your development if you can train your neural network in an hour rather than in months of work. That makes faster computation an important help in the development of the field.
